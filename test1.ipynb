{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # color conversion\n",
    "    image.flags.writeable = False # img no longer writeable\n",
    "    pred = model.process(image) # make landmark prediction\n",
    "    image.flags.writeable = True  # img now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # color reconversion\n",
    "    return image, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                              mp_drawing.DrawingSpec(color=(250,0,0), thickness=1, circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=1, circle_radius=0))\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(0,150,0), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(200,56,12), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(250,56,12), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_coordinates(results):\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros((468, 3))\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros((33, 3))\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21, 3))\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21, 3))\n",
    "    return np.concatenate([face, lh, pose, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        sign_map = json.load(f)\n",
    "    return sign_map\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    data_dir = \"\"\n",
    "    sequence_length = 12\n",
    "    rows_per_frame = 543\n",
    "\n",
    "\n",
    "ROWS_PER_FRAME = 543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "\n",
    "sign_map = load_json_file(CFG.data_dir + 'sign_to_prediction_index_map.json')\n",
    "# train_data = pd.read_csv(CFG.data_dir + 'train.csv')\n",
    "\n",
    "s2p_map = {k.lower(): v for k, v in load_json_file(CFG.data_dir + \"sign_to_prediction_index_map.json\").items()}\n",
    "p2s_map = {v: k for k, v in load_json_file(CFG.data_dir + \"sign_to_prediction_index_map.json\").items()}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_asl():\n",
    "    interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "    found_signatures = list(interpreter.get_signature_list().keys())\n",
    "    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "    sequence_data = []\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    sign = 0\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            draw(image, results)\n",
    "\n",
    "            cv2.putText(image, f\"Prediction:    {len(sequence_data)} {decoder(sign)}\", (3, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            landmarks = extract_coordinates(results)\n",
    "            sequence_data.append(landmarks)\n",
    "            if len(sequence_data) % 15 == 0:\n",
    "                prediction = prediction_fn(inputs=np.array(sequence_data, dtype = np.float32))\n",
    "                sequence_data = []\n",
    "                sign = np.argmax(prediction[\"outputs\"])\n",
    "                print(decoder(sign))\n",
    "                \n",
    "            # image = cv2.flip(image, 1)\n",
    "            cv2.imshow('Webcam Feed', image)\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud\n",
      "shower\n",
      "cloud\n",
      "shhh\n",
      "shhh\n",
      "look\n",
      "look\n",
      "helicopter\n",
      "airplane\n",
      "look\n",
      "horse\n",
      "TV\n"
     ]
    }
   ],
   "source": [
    "real_time_asl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
